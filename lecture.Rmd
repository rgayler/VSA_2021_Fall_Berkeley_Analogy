---
title: "Analogical Reasoning"
author: "Ross Gayler"
date: "2021-10-06"
output: 
  ioslides_presentation:
    widescreen: true
    incremental: false
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Motivation & Objectives

Analogy is really cool and central to cognition

Analogy is a good use case for the unique properties of VSA/HDC

-   What makes analogy hard for conventional computing?
-   Which VSA/HDC features might help with analogy?
-   Not a solved problem

Use a set of attempts at aspects of analogy to highlight some VSA design
issues

## Outline

What is analogy?

Why is analogy hard for conventional computing?

VSA design examples:

-   Plate - Similarity of hand crafted vectors
-   Mikolov - Similarity of learned word vectors
-   Kanerva - Simple substitution
-   Emruli - Substitution with lookup
-   Gayler - Settling on substitution

## What is *ANALOGY*?

*ANALOGY* $\triangleq$ what analogy *really* is

Whatever it is, *ANALOGY* as a cognitive phenomenon is a complex,
nuanced thing

-   Everybody presents a different partial view of *ANALOGY*

    -   Tendency to interpret the partial view as the whole thing

        -   Analogical reasoning
        -   Proportional analogies
        -   Grand analogy (analogy as a party trick)

    -   Please don't do that

*ANALOGY* is too big to fit in this lecture, so I will resort to
assertions and hand waving to explain enough of it for current purposes

## Analogy is the core of cognition

Quote from Hofstadter (2006):

analogy-making $\triangleq$\
the perception of common essence^1^\
between two things^2^

<font size="4"> ^1^ In one's current frame of mind\
^2^ Thing $\triangleq$ mental thing </font>

See also\
Gust et al (2008)\
Chalmers et al (1992)\
Blokpoel et al (2018)

I will jump off from Blokpoel:\
cognition as inference to the best explanation

## Inference to the Best Explanation

The cognitive loop:\
Given some inputs (evidence $e$) and a set of potential explanations
(hypotheses $H$) find the hypothesis ($h$) that best explains the
evidence

Evidence and hypotheses are represented relationally (trees/graphs)

-   This is effectively a bet that natural regularities are "best"
    captured as transformations

"explains" is interpreted as graph structure matching - (sub)graph
isomorphism

-   "common essence" = common relational structure

Partial structure matching enables inference by carrying structure from
one representation to another (think of it as pattern completion via
autoassociative memory)

## How are hypotheses generated?

Hypotheses are generated from *all* the agent's relevant knowledge

The hypothesis space must be open-ended, to allow for novelty

-   Hypotheses must be compositional

    -   Allows infinite productivity
    -   Allows novel compositions of familiar components
    -   Like a grammar for hypotheses

## Example: Relational representation

solar system = base structure = hypothesis (on this slide)\
atom = target structure = evidence (on this slide)\
structural similarity = literal similarity given appropriate systematic
substitution

![<font size="1">Chalmers et al
(1992)</font>](assets/chalmers-analogy_graphs.png){width="60%"}

## Example: Relational representation of evidence

![<font size="1">Blokpoel et al
(2018)</font>](assets/blokpoel-evidence.png){width="100%"}

## Example: Relational representation of knowledge

![<font size="1">Blokpoel et al
(2018)</font>](assets/blokpoel-knowledge.png){width="38%"}

## Example: Analogical augmentation

![<font size="1">Blokpoel et al
(2018)</font>](assets/blokpoel-augmentation_2.png){width="80%"}

## Example: Augmentation of evidence

![<font size="1">Blokpoel et al
(2018)</font>](assets/blokpoel-augmentation_1.png){width="70%"}

## Example: Explanation

![<font size="1">Blokpoel et al
(2018)</font>](assets/blokpoel-explanation_2.png){width="72%"}

## Why is analogy hard for conventional computing?

Subgraph isomorphism (of two graphs) is NP-complete

-   Considers all possible vertex mappings

-   The "obvious" approach is brute-force exhaustive enumeration

-   Each vertex mapping provides very little information about the
    adequacy of the other vertex mappings

Considering all the base structures in the agent's knowledge is much
larger

Considering the transitive closure of analogical augmentations is much
larger

## Which VSA features might help with analogy?

Preview only at this point

-   Hardware parallelism

    -   Operations are generally elementwise
    -   Small fan-in per element

-   Mathematical parallelism

    -   Distributive parallelism

        -   A \* (B + C + ...) = A*B + A*C + ...

    -   Equational parallelism

        -   T = A + B + C = P + Q + R + S = X \* Y \* Z

    -   The hardware only "sees" the total vector

## Plate - Hand crafted similarity

Focus of Chapter 6 of Plate's thesis (1994) is the adequacy of
dot-product similarity as a measure of structural similarity of
representations

Reports experiments with hand-crafted representations aimed at
qualitatively reproducing the results of psychology research into human
judgement of analogical similarity under varying contributions of
component similarity to overall similarity.

My take,

-   superficial similarity $\overset{\operatorname{very}}{\approx}$
    similarity of arguments of relations
-   structural similarity $\overset{\operatorname{very}}{\approx}$
    similarity of pattern of relations

but researchers are free to suit the details of their definitions to
their needs

## Example stimuli <font size="1">(Plate, 2000)</font>

**P** (Probe) "Spot bit Jane, causing Jane to flee from Spot"

**LS** (Literal Similarity) "Fido bit John, causing John to flee from
Fido." (Has both structural and superficial similarity to the probe P.)

**SF** (Surface features) "John fled from Fido, causing Fido to bite
John." (Has superficial but not structural similarity.)

**CM** (Cross-mapped analogy) "Fred bit Rover, causing Rover to flee
from Fred." (Has both structural and superficial similarity, but types
of corresponding objects are switched.)

**AN** (Analogy) "Mort bit Felix, causing Felix to flee from Mort." (Has
structural but not superficial similarity).

**FOR** (First-order-relations only) "Mort fled from Felix, causing
Felix to bite Mort." (Has neither structural nor superficial similarity,
other than shared predicates.)

## Base and token vectors

![<font size="1">Plate
(1994)</font>](assets/plate-base_token.png){width="74%"}

## Episode construction - Probe vector

"Spot bit Jane, causing Jane to flee from Spot"

![<font size="1">Plate
(1994)</font>](assets/plate-probe_vector.png){width="100%"}

Note the addition of "lower level" components to the representations of
"higher level" composites

Construction of all the episode vectors follows the same scheme

## Dot-product similarity with Probe

![<font size="1">Plate
(2000)</font>](assets/plate-similarities.png){width="100%"}

## Reminders of dot-product similarity

$sim(A, A') = sim(A, A + X) \gt 0$

$sim((P \otimes A), (P \otimes A')) = sim(A, A') \gt 0$

$sim((P \otimes A), (P' \otimes A')) = sim(P, P') \times sim(A, A') \gt 0$

$sim((A \otimes B \otimes C), (A' \otimes B \otimes C \otimes P)) \approx 0$

If using self-inverse products:

$sim((A \otimes B \otimes C), (A' \otimes B \otimes C \otimes P) \otimes P) = sim(A, A')$

## My interpretation

Representation of structure *requires* product operations,\
which destroy dot-product similarity of result to arguments

-   structural similarity $\neq$ dot-product similarity of core
    structure

Plate decorates the core structures with components to create similarity

-   Might be *ad hoc* (depends on whetehr it is natural for the
    contruction process)

-   Might be missing necessary structure

    -   Predicates are not represented as unique instances,

        -   "Spot bit Fido causing Felix to bite John"

    -   but representing them as unique instances would destroy their
        similarity

        -   $sim((bite_1 \otimes bite_{agt} \otimes X), (bite_2 \otimes bite_{agt} \otimes X)) \approx 0$

## Dot-product similarity is local

Dot-product similarity is at the heart of VSA system dynamics

Dot-product similarity is "local"

-   Almost all vectors are quasi-orthogonal to direction of interest
-   Similarity doesn't discriminate between them
-   Therefore similarity driven dynamics can't select between orthogonal
    directions

Relational structure is encoded via Multiply and Permute, which are
randomising (orthogonalising)

-   Therefore, something needs to be done to transform relational
    distance into similarity so that it can engage the dynamics

## ???

Problem of two - representations are naked

Needs a substitution to make the similarities visible

## Mikolov - Learned word vectors

## Kanerva - Simple substitution

## Emruli - Substitution with lookup

## Gayler - Settling on substitution

## References / Reading

M. Blokpoel, T. Wareham, P. Haselager, and I. van Rooij (2018) *Deep
Analogical Inference as the Origin of Hypotheses*. The Journal of
Problem Solving

D. J. Chalmers, R. M. French, and D. R. Hofstadter (1992) *High-level
perception, representation, and analogy: A critique of artificial
intelligence methodology*. Journal of Experimental & Theoretical
Artificial Intelligence

B. Emruli, R. W. Gayler, and F. Sandin (2013) *Analogical mapping and
inference with binary spatter codes and sparse distributed memory*. The
2013 International Joint Conference on Neural Networks (IJCNN)

B. Emruli and F. Sandin (2014) Analogical *Mapping with Sparse
Distributed Memory: A Simple Model that Learns to Generalize from
Examples*. Cognitive Computation

------------------------------------------------------------------------

R. W. Gayler and S. D. Levy (2009) *A Distributed Basis for Analogical
Mapping*. New Frontiers in Analogy Research, Proceedings of the Second
International Conference on Analogy, ANALOGY-2009

R. W. Gayler and R. Wales (1998) *Connections, Binding, Unification and
Analogical Promiscuity*. Advances In Analogy Research: Integration Of
Theory And Data From The Cognitive, Computational, And Neural Sciences

H. Gust, U. Krumnack, K.-U. Kühnberger, and A. Schwering (2008)
*Analogical Reasoning: A Core of Cognition*. KI - Künstliche Intelligenz

D. R. Hofstadter (2006) *Analogy as the core of cognition*. Stanford
Presidential Lecture

P. Kanerva (2000) *Large Patterns Make Great Symbols: An Example of
Learning from Example*. Hybrid Neural Systems

------------------------------------------------------------------------

P. Kanerva (2010) *What We Mean when We Say "What's the Dollar of
Mexico?": Prototypes and Mapping in Concept Space*. Quantum Informatics
2010: AAAI-Fall 2010 Symposium on Quantum Informatics for Cognitive,
Social, and Semantic Processes

S. D. Levy and R. W. Gayler (2009) *"Lateral inhibition" in a fully
distributed connectionist architecture*. In Proceedings of the Ninth
International Conference on Cognitive Modeling (ICCM 2009)

T. Mikolov, W. Yih, and G. Zweig (2013) *Linguistic Regularities in
Continuous Space Word Representations*. Proceedings of NAACL-HLT 2013

T. A. Plate (1994) *Distributed Representations and Nested Compositional
Structure*. PhD Thesis. University of Toronto

T. A. Plate (2000) *Analogy retrieval and processing with distributed
vector representations*. Expert Systems

------------------------------------------------------------------------

A. Rogers, A. Drozd, and L. Bofang (2017) *The (too Many) Problems of
Analogical Reasoning with Word Vectors*. Proceedings of the 6th Joint
Conference on Lexical and Computational Semantics (\*SEM 2017)
